{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c11bd2db-c549-4535-bcff-ecaf4733910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c652bc-6a42-47d8-b91b-3c58a0f8db60",
   "metadata": {},
   "source": [
    "## Génération des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e9fdd8d4-ef3e-4b9f-89ee-0c5b5016e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(a, b, c, idx):\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = a * x**2 + b * x + c\n",
    "    \n",
    "    plt.figure(figsize=(2, 2), dpi=32)\n",
    "    plt.plot(x, y, color='black', linewidth=3)\n",
    "    plt.ylim(-200, 200)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    path = f\"images/img_{idx:03}.png\"\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "40439c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.tri import Triangulation\n",
    "\n",
    "def generate_3dim_image(a, b, c, ax, idx, rotation):\n",
    "    \"\"\"Generate 3D plot on given axis instead of creating new figure\"\"\"\n",
    "    x = np.linspace(-6, 6, 30)\n",
    "    y = np.linspace(-6, 6, 30)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = a * (X + Y)**2 + b * (X + Y) + c\n",
    "    \n",
    "    tri = Triangulation(X.ravel(), Y.ravel())\n",
    "    \n",
    "    ax.plot_trisurf(tri, Z.ravel(), cmap='cool', edgecolor='none', alpha=0.8)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.pane.set_visible(False)\n",
    "    ax.yaxis.pane.set_visible(False)\n",
    "    ax.zaxis.pane.set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    \n",
    "    match rotation:\n",
    "        case 'XY':\n",
    "            ax.set_title('Surface Plot from XY')\n",
    "            ax.view_init(elev=90, azim=-90)\n",
    "        case 'XZ':\n",
    "            ax.set_title('Surface Plot from XZ')\n",
    "            ax.view_init(elev=0, azim=0)\n",
    "        case 'YZ':\n",
    "            ax.set_title('Surface Plot from YZ')\n",
    "            ax.view_init(elev=0, azim=-90)\n",
    "\n",
    "def create_combined_3d_plot(a, b, c, idx):\n",
    "    \"\"\"Create combined 3D plot and save it\"\"\"\n",
    "    # Create the subplot figure with 3 3D axes\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), subplot_kw={'projection': '3d'})\n",
    "    \n",
    "    # Plot the three rotations on the respective axes\n",
    "    generate_3dim_image(a, b, c, axes[0], idx, 'XY')\n",
    "    generate_3dim_image(a, b, c, axes[1], idx, 'XZ')\n",
    "    generate_3dim_image(a, b, c, axes[2], idx, 'YZ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    path = f\"images3dim/img_{idx:03}.png\"\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57ede5-4fee-454e-95b0-5b4d3e01bc53",
   "metadata": {},
   "source": [
    "## Génération du fichier paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ab16aa2-cf76-4498-94f5-97ee837d504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_excel(param_list, path='params.xlsx'):\n",
    "    df = pd.DataFrame(param_list, columns=['a', 'b', 'c'])\n",
    "    df.to_excel(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803900fe-30aa-48f1-8e5e-e301c1c5f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_images_from_excel(excel_path='params.xlsx'):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        generate_image(row['a'], row['b'], row['c'], idx)\n",
    "\n",
    "def generate_new_images_from_excel(excel_path='params.xlsx'):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        create_combined_3d_plot(row['a'], row['b'], row['c'], idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a026e11-5ff0-4410-bb3e-15097923c068",
   "metadata": {},
   "source": [
    "## Construction du MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f26de4e0-889b-4ff7-8443-29e3e2741eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a25fbc93-ed5b-4e68-a2f6-e127e60b9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRegressionDataset(Dataset):\n",
    "    def __init__(self, excel_path, image_folder):\n",
    "        self.df = pd.read_excel(excel_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),  \n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        params = self.df.iloc[idx][['a', 'b', 'c']].values.astype(np.float32)\n",
    "        img_path = os.path.join(self.image_folder, f\"img_{idx:03}.png\")\n",
    "        image = Image.open(img_path) #.convert('L')  \n",
    "        image = self.transform(image)\n",
    "        return torch.tensor(params), image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4f160af5-5cf5-44ea-869b-7c35f9f0287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 3\n",
    "output_size = 128\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_params, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, output_size * output_size * 3), # 3 for rgb\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        batch_size = x.shape[0]\n",
    "        print(output.reshape(batch_size, output_size, output_size, 3).shape)\n",
    "        return output.reshape(batch_size, output_size, output_size, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "04eb51ba-0a1f-4631-82a3-1db907d098d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training(training_data='images'):\n",
    "    dataset = ImageRegressionDataset('params.xlsx',training_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    model = MLP()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    patience = 5\n",
    "    best = 1e5\n",
    "    for epoch in range(100):\n",
    "        for inputs, targets in dataloader:\n",
    "            targets = targets.view(inputs.size(0), -1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if loss > best:\n",
    "                patience -= 1\n",
    "        else:\n",
    "            patience = 5\n",
    "            best = loss\n",
    "        if patience == 0:\n",
    "            torch.save(model.state_dict(), f'mlp_weights_epoch{epoch+1}.pth')\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), 'mlp_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c8365-6a8a-46d4-886e-0d2b67b801f4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6b31a-b3d5-4429-85b3-09464e1029af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_inference(a, b, c, weights='mlp_weights.pth',output_folder='predictions'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    model = MLP()\n",
    "    model.load_state_dict(torch.load(weights))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([[a, b, c]], dtype=torch.float32)\n",
    "        output = model(input_tensor).view(128, 128).numpy()\n",
    "        output[output < 0.8] *= 0.2 \n",
    "        plt.imshow(output, cmap='gray') # a modifier\n",
    "        plt.axis('off')\n",
    "        image_path = os.path.join(output_folder, f\"prediction_a{a}_b{b}_c{c}.png\")\n",
    "        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Image sauvegardée dans le fichier predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd35581-f172-4fc6-8511-d1080a661ce3",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddcc011e-5a24-40e1-baac-bfc4c0651033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 56.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 0.0145\n",
      "Epoch 2, Loss = 0.0114\n",
      "Epoch 3, Loss = 0.0105\n",
      "Epoch 4, Loss = 0.0086\n",
      "Epoch 5, Loss = 0.0079\n",
      "Epoch 6, Loss = 0.0068\n",
      "Epoch 7, Loss = 0.0059\n",
      "Epoch 8, Loss = 0.0049\n",
      "Epoch 9, Loss = 0.0054\n",
      "Epoch 10, Loss = 0.0043\n",
      "Epoch 11, Loss = 0.0043\n",
      "Epoch 12, Loss = 0.0029\n",
      "Epoch 13, Loss = 0.0032\n",
      "Epoch 14, Loss = 0.0029\n",
      "Epoch 15, Loss = 0.0027\n",
      "Epoch 16, Loss = 0.0026\n",
      "Epoch 17, Loss = 0.0021\n",
      "Epoch 18, Loss = 0.0022\n",
      "Epoch 19, Loss = 0.0017\n",
      "Epoch 20, Loss = 0.0023\n",
      "Epoch 21, Loss = 0.0017\n",
      "Epoch 22, Loss = 0.0015\n",
      "Epoch 23, Loss = 0.0012\n",
      "Epoch 24, Loss = 0.0018\n",
      "Epoch 25, Loss = 0.0018\n",
      "Epoch 26, Loss = 0.0017\n",
      "Epoch 27, Loss = 0.0011\n",
      "Epoch 28, Loss = 0.0012\n",
      "Epoch 29, Loss = 0.0013\n",
      "Epoch 30, Loss = 0.0009\n",
      "Epoch 31, Loss = 0.0014\n",
      "Epoch 32, Loss = 0.0006\n",
      "Epoch 33, Loss = 0.0014\n",
      "Epoch 34, Loss = 0.0009\n",
      "Epoch 35, Loss = 0.0009\n",
      "Epoch 36, Loss = 0.0014\n",
      "Epoch 37, Loss = 0.0010\n",
      "Epoch 38, Loss = 0.0016\n",
      "Epoch 39, Loss = 0.0007\n",
      "Epoch 40, Loss = 0.0009\n",
      "Epoch 41, Loss = 0.0013\n",
      "Epoch 42, Loss = 0.0007\n",
      "Epoch 43, Loss = 0.0005\n",
      "Epoch 44, Loss = 0.0004\n",
      "Epoch 45, Loss = 0.0010\n",
      "Epoch 46, Loss = 0.0010\n",
      "Epoch 47, Loss = 0.0008\n",
      "Epoch 48, Loss = 0.0004\n",
      "Epoch 49, Loss = 0.0004\n",
      "Epoch 50, Loss = 0.0003\n",
      "Epoch 51, Loss = 0.0005\n",
      "Epoch 52, Loss = 0.0008\n",
      "Epoch 53, Loss = 0.0006\n",
      "Epoch 54, Loss = 0.0005\n",
      "Epoch 55, Loss = 0.0008\n",
      "Epoch 56, Loss = 0.0003\n",
      "Epoch 57, Loss = 0.0002\n",
      "Epoch 58, Loss = 0.0004\n",
      "Epoch 59, Loss = 0.0004\n",
      "Epoch 60, Loss = 0.0007\n",
      "Epoch 61, Loss = 0.0003\n",
      "Epoch 62, Loss = 0.0010\n",
      "Epoch 63, Loss = 0.0008\n",
      "Epoch 64, Loss = 0.0005\n",
      "Epoch 65, Loss = 0.0012\n",
      "Epoch 66, Loss = 0.0008\n",
      "Epoch 67, Loss = 0.0006\n",
      "Epoch 68, Loss = 0.0007\n",
      "Epoch 69, Loss = 0.0015\n",
      "Epoch 70, Loss = 0.0002\n",
      "Epoch 71, Loss = 0.0007\n",
      "Epoch 72, Loss = 0.0006\n",
      "Epoch 73, Loss = 0.0008\n",
      "Epoch 74, Loss = 0.0002\n",
      "Epoch 75, Loss = 0.0003\n",
      "Epoch 76, Loss = 0.0002\n",
      "Epoch 77, Loss = 0.0003\n",
      "Epoch 78, Loss = 0.0004\n",
      "Epoch 79, Loss = 0.0003\n",
      "Epoch 80, Loss = 0.0006\n",
      "Epoch 81, Loss = 0.0004\n",
      "Epoch 82, Loss = 0.0002\n",
      "Epoch 83, Loss = 0.0002\n",
      "Epoch 84, Loss = 0.0007\n",
      "Epoch 85, Loss = 0.0007\n",
      "Epoch 86, Loss = 0.0002\n",
      "Epoch 87, Loss = 0.0006\n",
      "Epoch 88, Loss = 0.0002\n",
      "Epoch 89, Loss = 0.0005\n",
      "Epoch 90, Loss = 0.0002\n",
      "Epoch 91, Loss = 0.0002\n",
      "Epoch 92, Loss = 0.0001\n",
      "Epoch 93, Loss = 0.0008\n",
      "Epoch 94, Loss = 0.0008\n",
      "Epoch 95, Loss = 0.0002\n",
      "Epoch 96, Loss = 0.0005\n",
      "Epoch 97, Loss = 0.0007\n",
      "Epoch 98, Loss = 0.0007\n",
      "Epoch 99, Loss = 0.0003\n",
      "Epoch 100, Loss = 0.0008\n"
     ]
    }
   ],
   "source": [
    "a_vals = np.linspace(1, 3, 10)\n",
    "b_vals = np.linspace(10, 30, 10)\n",
    "c_vals = np.linspace(1, 3, 10)\n",
    "param_list = [(a, b, c) for a in a_vals for b in b_vals for c in c_vals]\n",
    "generate_excel(param_list)\n",
    "generate_all_images_from_excel()\n",
    "main_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f0539934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 0.0078\n",
      "Epoch 2, Loss = 0.0027\n",
      "Epoch 3, Loss = 0.0007\n",
      "Epoch 4, Loss = 0.0004\n",
      "Epoch 5, Loss = 0.0001\n",
      "Epoch 6, Loss = 0.0002\n",
      "Epoch 7, Loss = 0.0001\n",
      "Epoch 8, Loss = 0.0001\n",
      "Epoch 9, Loss = 0.0001\n",
      "Epoch 10, Loss = 0.0001\n",
      "Epoch 11, Loss = 0.0002\n",
      "Epoch 12, Loss = 0.0001\n",
      "Epoch 13, Loss = 0.0001\n",
      "Epoch 14, Loss = 0.0001\n",
      "Epoch 15, Loss = 0.0001\n",
      "Epoch 16, Loss = 0.0001\n",
      "Epoch 17, Loss = 0.0001\n",
      "Epoch 18, Loss = 0.0001\n",
      "Epoch 19, Loss = 0.0001\n",
      "Epoch 20, Loss = 0.0000\n",
      "Epoch 21, Loss = 0.0000\n",
      "Epoch 22, Loss = 0.0001\n",
      "Epoch 23, Loss = 0.0001\n",
      "Epoch 24, Loss = 0.0000\n",
      "Epoch 25, Loss = 0.0001\n",
      "Epoch 26, Loss = 0.0001\n",
      "Epoch 27, Loss = 0.0000\n",
      "Epoch 28, Loss = 0.0002\n",
      "Epoch 29, Loss = 0.0000\n",
      "Epoch 30, Loss = 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#generate_new_images_from_excel()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m main_training(training_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages3dim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[193], line 23\u001b[0m, in \u001b[0;36mmain_training\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m     21\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 23\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#generate_new_images_from_excel()\n",
    "main_training(training_data='images3dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6ade1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 65536])) that is different to the input size (torch.Size([8, 128, 128, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (65536) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_training(training_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages3dim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[237], line 14\u001b[0m, in \u001b[0;36mmain_training\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (65536) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "main_training(training_data='images3dim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e4cc8-3ff4-4cc0-8616-c8db1a086e8f",
   "metadata": {},
   "source": [
    "## Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5b35da6f-0c61-49a3-80ec-0b4979a96a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sauvegardée dans le fichier predictions\n"
     ]
    }
   ],
   "source": [
    "main_inference(8,2,3, weights='mlp_weights_epoch20.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
